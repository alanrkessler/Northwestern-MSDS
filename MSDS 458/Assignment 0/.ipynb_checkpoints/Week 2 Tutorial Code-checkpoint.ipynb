{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transfer_fcn(summed_neuron_input, alpha):\n",
    "    \"\"\"Compute neuron activation using sigmoid transfer function\"\"\"\n",
    "    activation = 1.0 / (1.0 + exp(-alpha*summed_neuron_input))\n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transfer_fcn_deriv(neuron_output, alpha):\n",
    "    \"\"\"Compute derivative of transfer function\"\"\"\n",
    "    return alpha*neuron_output*(1.0 - neuron_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_neural_net_size_specs():\n",
    "    \"\"\"Returns a list of three values:\n",
    "          the input, hidden, and output sizes.\"\"\"\n",
    "    num_input_nodes = 2\n",
    "    num_hidden_nodes = 2\n",
    "    num_output_nodes = 2\n",
    "    print(\"This network is set up to run the X-OR problem.\")\n",
    "    print(\"The numbers of nodes in the input, hidden, and output layers have been set to 2 each.\")\n",
    "    array_size_list = (num_input_nodes, num_hidden_nodes, num_output_nodes)\n",
    "    return array_size_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weight():\n",
    "    \"\"\"Initialize a specific connection weight with random number\"\"\"\n",
    "    random_num = random.random()\n",
    "    weight = 1 - 2*random_num           \n",
    "    return weight  \n",
    "\n",
    "def initialize_weight_array (weight_array_size_list, debug_off):\n",
    "    \"\"\"Uses the size list to initialize connection weight arrays\"\"\"\n",
    "    num_bottom_nodes = weight_array_size_list[0]\n",
    "    num_upper_nodes = weight_array_size_list[1]\n",
    "            \n",
    "    # Initialize the weight variables with random weights\n",
    "    wt00 = initialize_weight()\n",
    "    wt01 = initialize_weight()\n",
    "    wt10 = initialize_weight()\n",
    "    wt11 = initialize_weight()    \n",
    "    weight_array = np.array([[wt00, wt10], [wt01, wt11]])\n",
    "    \n",
    "    if not debug_off:\n",
    "        print(\"Initialized weights are:\")\n",
    "        print(f\"weight00 = {wt00:.4f}\")\n",
    "        print(f\"weight01 = {wt01:.4f}\")\n",
    "        print(f\"weight10 = {wt10:.4f}\")\n",
    "        print(f\"weight11 = {wt11:.4f}\")        \n",
    "        print(\"\")\n",
    "        print(\"The weight array is:\")\n",
    "        print(weight_array)\n",
    "        print(\"weight00 = {(weightArray[0,0], weightArray[0,1]):.4f}\")\n",
    "        print(\"weight01 = {(weightArray[1,0], weightArray[1,1]):.4f}\")\n",
    "\n",
    "    return weight_array  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_bias_weight_array():\n",
    "    \"\"\"Initialized bias weight array \n",
    "       using number of nudes of any two layers\"\"\"\n",
    "    \n",
    "    bias_weight0 = initialize_weight()\n",
    "    bias_weight1 = initialize_weight()\n",
    "    bias_weight_array = np.array([bias_weight0, bias_weight1])     \n",
    "    return bias_weight_array  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtainRandomXORTrainingValues ():\n",
    "    \"\"\"Obtain 4 randomly-selected training data set list\n",
    "       for the X-OR problem\"\"\"\n",
    "   \n",
    "    training_data_set_num = random.randint(1, 4)\n",
    "    if training_data_set_num == 1:\n",
    "        training_data_list = (0, 0, 0, 1, 0)\n",
    "    elif training_data_set_num == 2:\n",
    "        training_data_list = (0,1, 1, 0, 1)\n",
    "    elif training_data_set_num == 3:\n",
    "        training_data_list = (1, 0, 1, 0, 2)\n",
    "    else:\n",
    "        training_data_list = (1, 1, 0, 1, 3)\n",
    "    return training_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_single_neuron_activation(alpha, wt0, wt1, input0, input1, bias, debug_off):\n",
    "    \"\"\"Compute neuron activation - summed weights inputs after passing through transfer function\"\"\"\n",
    "    \n",
    "    # Obtain the inputs into the neuron; this is the sum of weights times inputs\n",
    "    summed_neuron_input = wt0*input0 + wt1*input1 + bias\n",
    "\n",
    "    # Pass the summedNeuronActivation and the transfer function parameter alpha into the transfer function\n",
    "    activation = compute_transfer_fcn(summed_neuron_input, alpha)\n",
    "\n",
    "    if not debug_off:        \n",
    "        print(\"In computeSingleNeuronActivation with input0, input 1 given as: \")\n",
    "        print(input0)\n",
    "        print(input1)\n",
    "        print(\"The summed neuron input is {summed_neuron_input:.4f}\")   \n",
    "        print(\"The activation (applied transfer function) for that neuron is {activation:.4f}\")    \n",
    "    \n",
    "    return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello 4\n"
     ]
    }
   ],
   "source": [
    "def compute_single_feed_forwardpass (alpha, input_data_list, w_weight_array, v_weight_array, \n",
    "                                     bias_hidden_weight_array, bias_output_weight_array, debug_off):\n",
    "    \"\"\"Perform a single feedforward pass\"\"\"\n",
    "\n",
    "    input0 = input_data_list[0]\n",
    "    input1 = input_data_list[1]      \n",
    "      \n",
    "    # Assign the input-to-hidden weights to specific variables\n",
    "    w_wt00 = w_weight_array[0,0]\n",
    "    w_wt10 = w_weight_array[0,1]\n",
    "    w_wt01 = w_weight_array[1,0]       \n",
    "    w_wt11 = w_weight_array[1,1]\n",
    "    \n",
    "    # Assign the hidden-to-output weights to specific variables\n",
    "    v_wt00 = v_weight_array[0,0]\n",
    "    v_wt10 = v_weight_array[0,1]\n",
    "    v_wt01 = v_weight_array[1,0]       \n",
    "    v_wt11 = v_weight_array[1,1]    \n",
    "    \n",
    "    bias_hidden0 = bias_hidden_weight_array[0]\n",
    "    bias_hidden1 = bias_hidden_weight_array[1]\n",
    "    bias_output0 = bias_hidden_weight_array[0]\n",
    "    bias_output1 = bias_hidden_weight_array[1]\n",
    "    \n",
    "    # Obtain the activations of the hidden nodes        \n",
    "    if not debug_off:\n",
    "        print(\"For hiddenActivation0 from input0, input1 =\")\n",
    "        print(input0)\n",
    "        print(input1)\n",
    "    hiddenActivation0 = compute_single_neuron_activation(alpha, w_wt00, w_wt10, \n",
    "                                                         input0, input1, bias_hidden0, debug_off)\n",
    "    hiddenActivation1 = compute_single_neuron_activation(alpha, w_wt01, w_wt11, \n",
    "                                                         input0, input1, bias_hidden1, debug_off)\n",
    "    if not debug_off: \n",
    "        print(\"\")\n",
    "        print '  In computeSingleFeedforwardPass: '\n",
    "        print '  Input node values: ', input0, ', ', input1\n",
    "        print '  The activations for the hidden nodes are:'\n",
    "        print '    Hidden0 = %.4f' % hiddenActivation0, 'Hidden1 = %.4f' % hiddenActivation1\n",
    "\n",
    "\n",
    "# Obtain the activations of the output nodes    \n",
    "    outputActivation0 = computeSingleNeuronActivation(alpha, vWt00, vWt10, hiddenActivation0, \n",
    "    hiddenActivation1, biasOutput0, debugComputeSingleNeuronActivationOff)\n",
    "    outputActivation1 = computeSingleNeuronActivation(alpha, vWt01, vWt11, hiddenActivation0, \n",
    "    hiddenActivation1, biasOutput1, debugComputeSingleNeuronActivationOff)\n",
    "    if not debugComputeSingleFeedforwardPassOff: \n",
    "        print ' '\n",
    "        print '  Computing the output neuron activations' \n",
    "        print ' '        \n",
    "        print '  Back in ComputeSingleFeedforwardPass (for hidden-to-output computations)'\n",
    "        print '  The activations for the output nodes are:'\n",
    "        print '    Output0 = %.4f' % outputActivation0, 'Output1 = %.4f' % outputActivation1\n",
    "\n",
    "\n",
    "               \n",
    "    actualAllNodesOutputList = (hiddenActivation0, hiddenActivation1, outputActivation0, outputActivation1)\n",
    "                                                                                                \n",
    "    return (actualAllNodesOutputList);\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
